{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFro9yyUgrw5J24JPkHm5S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikagithub/Machine-Learning-All-Algorithms/blob/main/AdaBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AdaBoost**\n",
        "\n",
        "The module sklearn.ensemble includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire [FS1995].\n",
        "\n",
        "The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consists of applying weights\n",
        ",\n",
        ", â€¦,\n",
        " to each of the training samples. Initially, those weights are all set to\n",
        ", so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly.\n",
        "AdaBoost can be used both for classification and regression problems:\n",
        "\n",
        "For multi-class classification, AdaBoostClassifier implements AdaBoost.SAMME [ZZRH2009].\n",
        "\n",
        "For regression, AdaBoostRegressor implements AdaBoost.R2 [D1997]."
      ],
      "metadata": {
        "id": "pozHd6xG3zZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyqYs6cA3p5Z",
        "outputId": "e69c68f7-12ef-4f47-88c2-904bb8b2ad7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9533333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "cif = AdaBoostClassifier(n_estimators=100, algorithm = \"SAMME\",)\n",
        "scores = cross_val_score(cif, X, y, cv=5)\n",
        "scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of weak learners is controlled by the parameter n_estimators. The learning_rate parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the estimator parameter. The main parameters to tune to obtain good results are n_estimators and the complexity of the base estimators (e.g., its depth max_depth or minimum required number of samples to consider a split min_samples_split)."
      ],
      "metadata": {
        "id": "orSYvs2r4y7I"
      }
    }
  ]
}